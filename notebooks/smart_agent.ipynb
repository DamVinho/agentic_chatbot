{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648cf255",
   "metadata": {},
   "source": [
    "## <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "\n",
    "Our goal is to build a chatbot from scratch with LangGraph, FastAPI & Streamlit. The conversation flows will be managed with LangGraph and we will use FastAPI to serve the chatbot endpoint. We will finnaly build a Streamlit UI to interact with the backend API.\n",
    "\n",
    "#### **Constraints**:\n",
    "\n",
    " - Use local **ollama**\n",
    "\n",
    "\n",
    "#### **Tools**:\n",
    "\n",
    " - None\n",
    "\n",
    "#### **Requirements**:\n",
    " - Make it work as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a37d2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfc233",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d4471",
   "metadata": {},
   "source": [
    "### **`Backend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5477f",
   "metadata": {},
   "source": [
    "#### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ad3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/utils/model_params.py\n",
    "# model variables\n",
    "MODEL_URL = \"http://ollama:11434\"\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "MODEL_TEMP = 0.0\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e007cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/utils/model_provider.py\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "#from utils.model_params import MODEL_URL, MODEL_NAME, MODEL_TEMP\n",
    "\n",
    "def load_model(url:str=MODEL_URL, model_name: str=MODEL_NAME, temperature:float=MODEL_TEMP, **kwargs):\n",
    "    \"\"\"Load local ollama model\"\"\"\n",
    "    return ChatOllama(base_url=url, model=model_name, temperature=temperature, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410626c",
   "metadata": {},
   "source": [
    "#### Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ed14a",
   "metadata": {},
   "source": [
    "##### State variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61203475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/core/state.py\n",
    "from typing import TypedDict, Dict, Any, Annotated, Sequence, Optional\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    session_id : Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64949acc",
   "metadata": {},
   "source": [
    "##### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d172cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/core/nodes.py\n",
    "\n",
    "#from core.state import ChatState\n",
    "#from utils.model_provider import load_model\n",
    "#from utils.model_params import MODEL_URL, MODEL_NAME, MODEL_TEMP, SYSTEM_PROMPT\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "class ChatNode:\n",
    "    def __init__(self, url:str=MODEL_URL,\n",
    "                 model_name:str=MODEL_NAME,\n",
    "                 temperature:float=MODEL_TEMP,\n",
    "                 system_prompt:str=SYSTEM_PROMPT):\n",
    "        self.systeme_message = SystemMessage(content=system_prompt)\n",
    "        self.model = load_model(url, model_name, temperature)\n",
    "\n",
    "    def run(self, state: ChatState) -> ChatState:\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "        if not messages or type(messages[0]) != SystemMessage:\n",
    "            messages.insert(0, self.systeme_message)\n",
    "\n",
    "        response = self.model.invoke(messages)\n",
    "        state[\"messages\"].append(response)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a56f7",
   "metadata": {},
   "source": [
    "##### The graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5288b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/core/graph.py\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "#from core.state import ChatState\n",
    "#from core.nodes import ChatNode\n",
    "from IPython.display import Image, display\n",
    "from typing import Optional\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class ChatGraph:\n",
    "    def __init__(self):\n",
    "        # nodes\n",
    "        self.chat_node = ChatNode()\n",
    "\n",
    "        # memory saver\n",
    "        self.memory_saver = MemorySaver()\n",
    "\n",
    "    def graph_builder(self):\n",
    "        graph_builder = StateGraph(ChatState)\n",
    "\n",
    "        # Nodes\n",
    "        graph_builder.add_node(\"chat_agent\", self.chat_node.run)\n",
    "\n",
    "        # Edges\n",
    "        graph_builder.add_edge(START, \"chat_agent\")\n",
    "\n",
    "        # return compiled graph\n",
    "        return graph_builder.compile(checkpointer=self.memory_saver)\n",
    "    \n",
    "    @property\n",
    "    def graph(self):\n",
    "        if hasattr(self, \"_graph\"):\n",
    "            return self._graph\n",
    "        self._graph = self.graph_builder()\n",
    "        return self._graph\n",
    "    \n",
    "    def display(self):\n",
    "        display(Image(self.graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "    def invoke(self, input:str, config: Optional[RunnableConfig]=None):\n",
    "        return self.graph.invoke(input, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f551fb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = ChatGraph()\n",
    "graph.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab4433",
   "metadata": {},
   "source": [
    "##### Data models for (backend) api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba61b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../backend/api/models.py\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_input: str\n",
    "    recursion_limit: int=Field(default=25)\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    session_id: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c7cb0",
   "metadata": {},
   "source": [
    "##### Backend logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../backend/api/main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "from api.models import ChatRequest, ChatResponse\n",
    "from core.graph import ChatGraph\n",
    "import uuid\n",
    "import traceback\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# define the app\n",
    "app = FastAPI(\n",
    "    title=\"Smart AI Chat Agent API\",\n",
    "    description=\"API for interacting with the smart chatbot\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# store chat sessions\n",
    "chat_sessions : Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# end points\n",
    "@app.post(\"/chat/start\", response_model=ChatResponse)\n",
    "async def start_chat(request: ChatRequest):\n",
    "    \"\"\"Start a new chat session with the chatbot\"\"\"\n",
    "    try:\n",
    "        # create a new graph instance\n",
    "        graph = ChatGraph()\n",
    "\n",
    "        # generate a unique session id\n",
    "        session_id = str(uuid.uuid4())\n",
    "\n",
    "        # run a chat by invoking the graph\n",
    "        inputs = {\"messages\":[{\"role\":\"user\",\n",
    "                               \"content\":request.user_input}]}\n",
    "        \n",
    "        config = {\n",
    "            \"recursion_limit\":request.recursion_limit,\n",
    "            \"configurable\":{\"thread_id\":session_id}\n",
    "            }\n",
    "\n",
    "        output = graph.invoke(\n",
    "            input=inputs,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # store the graph instance\n",
    "        chat_sessions[session_id] = {\n",
    "            \"graph\": graph,\n",
    "            \"state\": output,\n",
    "            \"config\": config\n",
    "        }\n",
    "\n",
    "        return ChatResponse(\n",
    "            response=output[\"messages\"][-1].content,\n",
    "            session_id=session_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error starting chat: {traceback.format_exc()}\")\n",
    "        raise HTTPException(status_code=500, detail=traceback.format_exc())\n",
    "    \n",
    "# continue chat\n",
    "@app.post(\"/chat/{session_id}/continue\", response_model=ChatResponse)\n",
    "async def continue_chat(session_id:str, request:ChatRequest):\n",
    "    if session_id not in chat_sessions:\n",
    "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
    "    \n",
    "    # retrieve session and add user input\n",
    "    chat_session = chat_sessions[session_id]\n",
    "    chat_session[\"state\"][\"messages\"].append(HumanMessage(content=request.user_input))\n",
    "\n",
    "    # submit to chat\n",
    "    new_state = chat_session[\"graph\"].invoke(chat_session[\"state\"], chat_session[\"config\"])\n",
    "\n",
    "    # update session\n",
    "    chat_sessions[session_id].update({\"state\": new_state})\n",
    "\n",
    "    return ChatResponse(\n",
    "        response=new_state[\"messages\"][-1].content,\n",
    "        session_id=session_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a646dee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0ed009",
   "metadata": {},
   "source": [
    "#### Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57fda8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../frontend/chatbot_ui.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../frontend/chatbot_ui.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# FastAPI server Url\n",
    "API_URL = \"http://127.0.0.1:8000\" # To change when deployed\n",
    "\n",
    "st.title(\"Smart chatbot\")\n",
    "\n",
    "# Session state for chat history\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = None\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# display chat history\n",
    "for message in st.session_state.messages:\n",
    "    role = \"user\" if message[\"role\"] == \"user\" else \"ai\"\n",
    "    st.chat_message(role).write(message[\"content\"])\n",
    "\n",
    "# user input field\n",
    "user_input = st.chat_input(\"Ask me anything\")\n",
    "\n",
    "if user_input:\n",
    "    # Add user message to chat history and display it directly\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    st.chat_message(\"user\").write(user_input)\n",
    "\n",
    "    # Detect if the session is new or not\n",
    "    if st.session_state.session_id is None:\n",
    "        # Start a new chat session\n",
    "        response = requests.post(f\"{API_URL}/chat/start\", json={\"user_input\":user_input})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            st.session_state.session_id = data.get(\"session_id\")\n",
    "            ai_response = data.get(\"response\")\n",
    "            st.session_state.messages.append({\"role\":\"ai\", \"content\":ai_response})\n",
    "            st.chat_message(\"ai\").write(ai_response)\n",
    "\n",
    "        else:\n",
    "            st.error(\"Failed to start chat session.\")\n",
    "    else:\n",
    "        # Continue an existing chat\n",
    "        response = requests.post(f\"{API_URL}/chat/{st.session_state.session_id}/continue\",\n",
    "                                 json={\"user_input\":user_input})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            ai_response = data.get(\"response\")\n",
    "            st.session_state.messages.append({\"role\":\"ai\", \"content\":ai_response})\n",
    "            st.chat_message(\"ai\").write(ai_response)\n",
    "        else:\n",
    "            st.error(\"Failed to continue chat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165a511",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_ollama_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
